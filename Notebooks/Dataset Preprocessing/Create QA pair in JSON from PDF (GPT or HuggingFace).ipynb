{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNxUH8dr6VEsTdjbrVYQoFc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7_Pkz_1m4wvo","executionInfo":{"status":"ok","timestamp":1752590185867,"user_tz":-120,"elapsed":14252,"user":{"displayName":"Kallal Nath","userId":"13092581678021689277"}},"outputId":"81665f50-c65a-42e8-a51a-6fb55c48b54b","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n","Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.7)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n","Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.95.1)\n","Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250506)\n","Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.2.1)\n","Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n","Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n","Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.9)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n","Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.9)\n","Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.95.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.9)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"]}],"source":["# Step 1: Setup and Installation\n","\n","# Install required libraries\n","!pip install PyPDF2 pdfplumber transformers torch datasets openai python-dotenv\n","!pip install sentence-transformers\n","!pip install --upgrade openai\n","\n","# Import libraries\n","import json\n","import PyPDF2\n","import pdfplumber\n","import re\n","from typing import List, Dict\n","import pandas as pd\n","from datetime import datetime"]},{"cell_type":"code","source":["# Step 2: Upload and Extract PDF Text\n","\n","# Upload PDF file to Colab\n","from google.colab import files\n","uploaded = files.upload()\n","\n","# Get the filename\n","pdf_filename = list(uploaded.keys())[0]\n","print(f\"Uploaded file: {pdf_filename}\")\n","\n","# Function to extract text from PDF\n","def extract_text_from_pdf(pdf_path):\n","    text_content = []\n","\n","    # Using pdfplumber for better text extraction\n","    with pdfplumber.open(pdf_path) as pdf:\n","        total_pages = len(pdf.pages)\n","        print(f\"Total pages: {total_pages}\")\n","\n","        for page_num, page in enumerate(pdf.pages):\n","            try:\n","                text = page.extract_text()\n","                if text:\n","                    text_content.append({\n","                        'page': page_num + 1,\n","                        'text': text.strip()\n","                    })\n","\n","                # Progress indicator\n","                if (page_num + 1) % 50 == 0:\n","                    print(f\"Processed {page_num + 1}/{total_pages} pages\")\n","\n","            except Exception as e:\n","                print(f\"Error processing page {page_num + 1}: {e}\")\n","                continue\n","\n","    return text_content\n","\n","# Extract text\n","print(\"Extracting text from PDF...\")\n","extracted_text = extract_text_from_pdf(pdf_filename)\n","print(f\"Successfully extracted text from {len(extracted_text)} pages\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":212},"id":"V8r6AIim5BCa","executionInfo":{"status":"ok","timestamp":1752590222419,"user_tz":-120,"elapsed":29716,"user":{"displayName":"Kallal Nath","userId":"13092581678021689277"}},"outputId":"d0fb30c3-2a0d-4d99-8cb8-46b11888de6e"},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-549b66da-d90b-460e-81a7-d0b8566fac78\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-549b66da-d90b-460e-81a7-d0b8566fac78\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Insurance_Handbook_20103.pdf to Insurance_Handbook_20103 (2).pdf\n","Uploaded file: Insurance_Handbook_20103 (2).pdf\n","Extracting text from PDF...\n","Total pages: 205\n","Processed 50/205 pages\n","Processed 100/205 pages\n","Processed 150/205 pages\n","Processed 200/205 pages\n","Successfully extracted text from 205 pages\n"]}]},{"cell_type":"code","source":["# Step 3: Text Preprocessing and Chunking\n","\n","# Function to clean and chunk text\n","def preprocess_and_chunk_text(text_data, chunk_size=500, overlap=200):\n","    chunks = []\n","\n","    for page_data in text_data:\n","        page_num = page_data['page']\n","        text = page_data['text']\n","\n","        # Clean text\n","        text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n","        text = re.sub(r'\\n+', '\\n', text)  # Remove extra newlines\n","        text = text.strip()\n","\n","        # Skip very short texts\n","        if len(text) < 100:\n","            continue\n","\n","        # Split into chunks\n","        words = text.split()\n","        for i in range(0, len(words), chunk_size - overlap):\n","            chunk_words = words[i:i + chunk_size]\n","            chunk_text = ' '.join(chunk_words)\n","\n","            if len(chunk_text.strip()) > 50:  # Only keep meaningful chunks\n","                chunks.append({\n","                    'page': page_num,\n","                    'chunk_id': len(chunks),\n","                    'text': chunk_text,\n","                    'word_count': len(chunk_words)\n","                })\n","\n","    return chunks\n","\n","# Process and chunk the text\n","print(\"Processing and chunking text...\")\n","text_chunks = preprocess_and_chunk_text(extracted_text)\n","print(f\"Created {len(text_chunks)} text chunks\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aaswDcv15I6S","executionInfo":{"status":"ok","timestamp":1752590319684,"user_tz":-120,"elapsed":47,"user":{"displayName":"Kallal Nath","userId":"13092581678021689277"}},"outputId":"d60313e8-0764-41d0-f687-dc5209f94921"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing and chunking text...\n","Created 327 text chunks\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eapQ5Qe6z1ql","executionInfo":{"status":"ok","timestamp":1752590326768,"user_tz":-120,"elapsed":1876,"user":{"displayName":"Kallal Nath","userId":"13092581678021689277"}},"outputId":"b0e56def-d6f5-4a5b-b9fc-248d1e2330b5"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Set the API key\n","\n","import openai\n","filepath = \"/content/drive/MyDrive/Master Thesis/Colab Notebook/\"\n","\n","with open(filepath + \"OpenAI_API_Key.txt\", \"r\") as f:\n","  openai.api_key = ' '.join(f.readlines())"],"metadata":{"id":"R9iAa3w_9Hhg","executionInfo":{"status":"ok","timestamp":1752590328776,"user_tz":-120,"elapsed":256,"user":{"displayName":"Kallal Nath","userId":"13092581678021689277"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Step 4: Generate Questions and Answers (old code for older versions of Open AI)\n","# Option A: Using OpenAI API\n","\n","import openai\n","from google.colab import userdata\n","\n","# Get API key from Colab secrets\n","openai.api_key = userdata.get('OPENAI_API_KEY')  # Add API key to Colab secrets\n","\n","\n","def generate_qa_with_openai(text_chunk, num_questions=5):\n","    prompt = f\"\"\"\n","Based on the following text, generate {num_questions} question-answer pairs.\n","Make the questions diverse (factual, analytical, conceptual) and ensure answers are accurate and complete.\n","\n","Text: {text_chunk}\n","\n","Format your response as JSON:\n","{{\n","  \"qa_pairs\": [\n","    {{\"question\": \"question here\", \"answer\": \"answer here\"}},\n","    {{\"question\": \"question here\", \"answer\": \"answer here\"}}\n","  ]\n","}}\n","\"\"\"\n","\n","    try:\n","        response = openai.ChatCompletion.create(\n","            model=\"gpt-3.5-turbo\",\n","            messages=[{\"role\": \"user\", \"content\": prompt}],\n","            temperature=0.7,\n","            max_tokens=1500\n","        )\n","\n","        result = json.loads(response.choices[0].message.content)\n","        return result.get('qa_pairs', [])\n","\n","    except Exception as e:\n","        print(f\"Error generating QA: {e}\")\n","        return []"],"metadata":{"id":"r6VFPdKA5QMX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 4: Generate Questions and Answers (new code for newer versions of OpenAI)\n","# Option A: Using OpenAI API v1.x.x\n","\n","import openai\n","import json\n","from openai import OpenAI\n","from google.colab import userdata\n","\n","# âœ… Set API key (retrieved securely from Colab Secrets)\n","client = OpenAI(api_key=userdata.get(\"OPENAI_API_KEY\"))\n","\n","def generate_qa_with_openai(text_chunk, num_questions=5):\n","    prompt = f\"\"\"\n","Based on the following text, generate {num_questions} question-answer pairs.\n","Make the questions diverse (factual, analytical, conceptual) and ensure answers are accurate and complete.\n","\n","Text: {text_chunk}\n","\n","Format your response as JSON:\n","{{\n","  \"qa_pairs\": [\n","    {{\"question\": \"question here\", \"answer\": \"answer here\"}},\n","    {{\"question\": \"question here\", \"answer\": \"answer here\"}}\n","  ]\n","}}\n","\"\"\"\n","\n","    try:\n","        response = client.chat.completions.create(\n","            model=\"gpt-3.5-turbo\",\n","            #model=\"gpt-4o\",\n","            #model=\"gpt-4o-mini\",\n","            messages=[{\"role\": \"user\", \"content\": prompt}],\n","            #temperature=0.7,\n","            temperature=0.4,\n","            max_tokens=1500\n","        )\n","\n","        content = response.choices[0].message.content.strip()\n","\n","        # Parse the JSON block from the model response\n","        result = json.loads(content)\n","        return result.get(\"qa_pairs\", [])\n","\n","    except Exception as e:\n","        print(f\"Error generating QA: {e}\")\n","        return []\n"],"metadata":{"id":"ck4-No1h4XXh","executionInfo":{"status":"ok","timestamp":1752590430463,"user_tz":-120,"elapsed":914,"user":{"displayName":"Kallal Nath","userId":"13092581678021689277"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# Option B: Using Hugging Face Transformers (Free alternative)\n","\n","from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","# Load question generation model\n","question_generator = pipeline(\"text2text-generation\",\n","                            model=\"valhalla/t5-base-qg-hl\",\n","                            tokenizer=\"valhalla/t5-base-qg-hl\")\n","\n","def generate_qa_with_transformers(text_chunk, num_questions=5):\n","    qa_pairs = []\n","\n","    # Split text into sentences for better question generation\n","    sentences = text_chunk.split('. ')\n","\n","    for i, sentence in enumerate(sentences[:num_questions]):\n","        if len(sentence.strip()) < 20:\n","            continue\n","\n","        try:\n","            # Generate question\n","            # This creates a prompt for the AI model. The model is trained to generate questions when it sees \"generate question: [text]\"\n","            input_text = f\"generate question: {sentence}\"\n","            question_result = question_generator(input_text,\n","                                               max_length=100,\n","                                               num_return_sequences=1)\n","            question = question_result[0]['generated_text'].strip()\n","\n","            # Use the original sentence/context as answer\n","            answer = sentence.strip()\n","\n","            if question and answer:\n","                qa_pairs.append({\n","                    \"question\": question,\n","                    \"answer\": answer\n","                })\n","\n","        except Exception as e:\n","            print(f\"Error generating question for sentence {i}: {e}\")\n","            continue\n","\n","    return qa_pairs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l2hqv94Y5jLc","executionInfo":{"status":"ok","timestamp":1752590358860,"user_tz":-120,"elapsed":1541,"user":{"displayName":"Kallal Nath","userId":"13092581678021689277"}},"outputId":"a4d0670e-300c-4781-9a4f-1ff89d8fb1dd"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n"]}]},{"cell_type":"code","source":["# Step 5: Generate Q&A Dataset (openai or transformers)\n","\n","def create_qa_dataset(text_chunks, method='openai', batch_size=10):\n","    dataset = []\n","\n","    print(f\"Generating Q&A pairs for {len(text_chunks)} chunks...\")\n","    print(\"Method used\", method)\n","\n","    for i, chunk in enumerate(text_chunks):\n","        try:\n","            # Choose generation method\n","            if method == 'openai':\n","                qa_pairs = generate_qa_with_openai(chunk['text'])\n","            else:\n","                qa_pairs = generate_qa_with_transformers(chunk['text'])\n","\n","            # Add metadata to each Q&A pair\n","            for qa in qa_pairs:\n","                dataset.append({\n","                    #'id': len(dataset),\n","                    'input': qa['question'],\n","                    'output': qa['answer'],\n","                    #'source_page': chunk['page'],\n","                    #'chunk_id': chunk['chunk_id'],\n","                    #'context': chunk['text'][:500] + \"...\" if len(chunk['text']) > 500 else chunk['text']\n","                })\n","\n","            # Progress update\n","            if (i + 1) % batch_size == 0:\n","                print(f\"Processed {i + 1}/{len(text_chunks)} chunks. Generated {len(dataset)} Q&A pairs so far.\")\n","\n","        except Exception as e:\n","            print(f\"Error processing chunk {i}: {e}\")\n","            continue\n","\n","    return dataset\n","\n","# Generate the dataset\n","qa_dataset = create_qa_dataset(text_chunks[:], method='openai')  # Start with first 100 chunks\n","print(f\"Generated {len(qa_dataset)} question-answer pairs\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X1VBPC9b5pm3","executionInfo":{"status":"ok","timestamp":1752591291494,"user_tz":-120,"elapsed":855778,"user":{"displayName":"Kallal Nath","userId":"13092581678021689277"}},"outputId":"e8c3b0de-5b61-4211-d03d-463cacda9c9f"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Generating Q&A pairs for 327 chunks...\n","Method used openai\n","Processed 10/327 chunks. Generated 50 Q&A pairs so far.\n","Processed 20/327 chunks. Generated 100 Q&A pairs so far.\n","Processed 30/327 chunks. Generated 150 Q&A pairs so far.\n","Processed 40/327 chunks. Generated 200 Q&A pairs so far.\n","Processed 50/327 chunks. Generated 250 Q&A pairs so far.\n","Processed 60/327 chunks. Generated 300 Q&A pairs so far.\n","Processed 70/327 chunks. Generated 350 Q&A pairs so far.\n","Processed 80/327 chunks. Generated 400 Q&A pairs so far.\n","Processed 90/327 chunks. Generated 450 Q&A pairs so far.\n","Processed 100/327 chunks. Generated 500 Q&A pairs so far.\n","Error generating QA: Expecting value: line 8 column 3 (char 1660)\n","Processed 110/327 chunks. Generated 545 Q&A pairs so far.\n","Processed 120/327 chunks. Generated 595 Q&A pairs so far.\n","Processed 130/327 chunks. Generated 645 Q&A pairs so far.\n","Processed 140/327 chunks. Generated 695 Q&A pairs so far.\n","Processed 150/327 chunks. Generated 745 Q&A pairs so far.\n","Processed 160/327 chunks. Generated 795 Q&A pairs so far.\n","Error generating QA: Expecting value: line 8 column 3 (char 1127)\n","Processed 170/327 chunks. Generated 840 Q&A pairs so far.\n","Processed 180/327 chunks. Generated 890 Q&A pairs so far.\n","Processed 190/327 chunks. Generated 940 Q&A pairs so far.\n","Processed 200/327 chunks. Generated 990 Q&A pairs so far.\n","Processed 210/327 chunks. Generated 1040 Q&A pairs so far.\n","Processed 220/327 chunks. Generated 1090 Q&A pairs so far.\n","Processed 230/327 chunks. Generated 1140 Q&A pairs so far.\n","Processed 240/327 chunks. Generated 1190 Q&A pairs so far.\n","Processed 250/327 chunks. Generated 1240 Q&A pairs so far.\n","Processed 260/327 chunks. Generated 1290 Q&A pairs so far.\n","Processed 270/327 chunks. Generated 1340 Q&A pairs so far.\n","Processed 280/327 chunks. Generated 1390 Q&A pairs so far.\n","Processed 290/327 chunks. Generated 1440 Q&A pairs so far.\n","Processed 300/327 chunks. Generated 1490 Q&A pairs so far.\n","Processed 310/327 chunks. Generated 1540 Q&A pairs so far.\n","Processed 320/327 chunks. Generated 1590 Q&A pairs so far.\n","Generated 1625 question-answer pairs\n"]}]},{"cell_type":"code","source":["# Step 6: Save Dataset as JSON\n","# Create final dataset structure\n","final_dataset = {\n","    'metadata': {\n","        'source_document': pdf_filename,\n","        'total_pages': len(extracted_text),\n","        'total_chunks_processed': len(text_chunks),\n","        'total_qa_pairs': len(qa_dataset),\n","        'generation_date': datetime.now().isoformat(),\n","        'generation_method': 'openai' # or 'transformers'\n","    },\n","    'qa_pairs': qa_dataset\n","}\n","\n","# Save to JSON file\n","output_filename = f\"qa_dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n","\n","with open(output_filename, 'w', encoding='utf-8') as f:\n","    json.dump(final_dataset, f, indent=2, ensure_ascii=False)\n","\n","print(f\"Dataset saved as: {output_filename}\")\n","\n","# Download the file\n","files.download(output_filename)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"W68omwzY5wfk","executionInfo":{"status":"ok","timestamp":1752591367171,"user_tz":-120,"elapsed":11,"user":{"displayName":"Kallal Nath","userId":"13092581678021689277"}},"outputId":"aab53cdc-3018-4c77-92dd-ec9a1e11f449"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset saved as: qa_dataset_20250715_145607.json\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_2a3383c3-4e49-4514-8e78-d20cc30fb0b3\", \"qa_dataset_20250715_145607.json\", 412800)"]},"metadata":{}}]},{"cell_type":"code","source":["# Step 7: Quality Check and Statistics. Analyze the generated dataset\n","\n","def analyze_dataset(dataset):\n","    qa_pairs = dataset['qa_pairs']\n","\n","    print(\"=== Dataset Analysis ===\")\n","    print(f\"Total Q&A pairs: {len(qa_pairs)}\")\n","\n","    # Question length statistics\n","    q_lengths = [len(qa['input'].split()) for qa in qa_pairs]\n","    print(f\"Average question length: {sum(q_lengths)/len(q_lengths):.1f} words\")\n","\n","    # Answer length statistics\n","    a_lengths = [len(qa['output'].split()) for qa in qa_pairs]\n","    print(f\"Average answer length: {sum(a_lengths)/len(a_lengths):.1f} words\")\n","\n","    # Page coverage\n","    #pages_covered = set(qa['source_page'] for qa in qa_pairs)\n","    #print(f\"Pages covered: {len(pages_covered)}\")\n","\n","    # Show sample Q&A pairs\n","    print(\"\\n=== Sample Q&A Pairs ===\")\n","    for i, qa in enumerate(qa_pairs[:3]):\n","        print(f\"\\nQ{i+1}: {qa['input']}\")\n","        print(f\"A{i+1}: {qa['output']}\")\n","        #print(f\"Source: Page {qa['source_page']}\")\n","\n","# Analyze the generated dataset\n","analyze_dataset(final_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4roMgx6e530C","executionInfo":{"status":"ok","timestamp":1752591389365,"user_tz":-120,"elapsed":17,"user":{"displayName":"Kallal Nath","userId":"13092581678021689277"}},"outputId":"d4709afd-a742-4498-9d40-0581f6a858f3"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["=== Dataset Analysis ===\n","Total Q&A pairs: 1625\n","Average question length: 10.8 words\n","Average answer length: 20.0 words\n","\n","=== Sample Q&A Pairs ===\n","\n","Q1: What is the purpose of an insurance handbook?\n","A1: The purpose of an insurance handbook is to provide information on what insurance is, what it does, and how it works.\n","\n","Q2: How can insurance help individuals and businesses?\n","A2: Insurance can help individuals and businesses by providing financial protection against potential risks and losses.\n","\n","Q3: What does the Insurance Information Institute do?\n","A3: The Insurance Information Institute is an organization that provides information and resources about insurance to the public.\n"]}]}]}